# LangChain en Cleo Agent: Orquestaci√≥n multi‚Äëmodelo, RAG y prompts de Cleo

Esta gu√≠a explica c√≥mo funciona la integraci√≥n de LangChain dentro de Cleo Agent, sus beneficios, y c√≥mo se conecta con RAG/embeddings y los prompts de Cleo, incluyendo el streaming compatible con la UI.

## Qu√© aporta LangChain
- Orquestaci√≥n multi‚Äëmodelo: elegir el mejor modelo por tarea para optimizar calidad/costo.
- Agentes especializados por proveedor: Groq para texto r√°pido y econ√≥mico; OpenAI para multimodal/razonamiento.
- Encadenamiento y herramientas: f√°cil de extender con nuevas capacidades.
- Tipado y m√©tricas: interfaces claras, logs y costos centralizados.

## Arquitectura en Cleo
- Router (`lib/langchain/router.ts`): decide el modelo seg√∫n tipo de tarea (texto/im√°genes), complejidad, etc.
- Agents (`lib/langchain/agents.ts`): instancia el LLM y procesa el input; inyecta contexto RAG y variantes de prompt de Cleo.
- Pipeline (`lib/langchain/pipeline.ts`): punto √∫nico de entrada. Normaliza metadata (useRAG, systemPromptVariant), llama al router y delega al agente.
- Tipos (`lib/langchain/types.ts`): contrato entre UI/API y la orquestaci√≥n (TaskInput/Output/Metadata).
- Endpoint (`app/api/multi-model-chat/route.ts`): expone la orquestaci√≥n con streaming SSE compatible con Vercel AI SDK v5.

Flujo: Request ‚Üí Pipeline ‚Üí Router ‚Üí Agent ‚Üí LLM ‚Üí Streaming a la UI.

### Vista detallada del flujo
1) UI env√≠a POST a `POST /api/chat`, que redirige a `POST /api/multi-model-chat` cuando eliges un modelo LangChain.
2) `multi-model-chat` valida el body con Zod, normaliza opciones y compone `metadata`:
	- `useRAG` se activa desde `options.enableSearch` (toggle de UI).
	- `systemPromptVariant` y `systemPrompt` personalizan el prompt de sistema.
3) `MultiModelPipeline.process` aplica defaults seguros y llama a `ModelRouter.route`.
4) El `AgentFactory` crea o reutiliza un agente seg√∫n el proveedor (reutilizaci√≥n mejora la latencia).
5) El agente construye el prompt de sistema con `getCleoPrompt(...)`, a√±ade contexto RAG si `useRAG` est√° activo, y llama al LLM.
6) La respuesta se devuelve con streaming SSE: `text-start` ‚Üí m√∫ltiples `text-delta` ‚Üí `finish` con uso de tokens.

## Modelos soportados
- Groq GPT‚Äë120OSS: texto general/function calling, muy r√°pido y barato.
- OpenAI GPT‚Äë4o‚Äëmini (o 5‚Äëmini): multimodal y mayor calidad para an√°lisis de documentos/im√°genes.

## RAG y embeddings
- Recuperaci√≥n: `lib/rag/retrieve.ts` y `lib/rag/embeddings.ts`.
- Activaci√≥n: desde la UI se usa `enableSearch`. El endpoint lo propaga como `metadata.useRAG`.
- Control: `metadata.maxContextChars` limita el tama√±o del contexto que se inserta.
- Depuraci√≥n: `metadata.debugRag` a√±ade logs con fuentes y tama√±os del contexto.

C√≥mo funciona:
1) Si `useRAG` es true, el agente consulta retrieve() con el mensaje del usuario.
2) Construye un bloque de contexto (trozos relevantes) y lo a√±ade al prompt del modelo.
3) Respeta el l√≠mite `maxContextChars` para evitar sobre‚Äëtokenizar.

Pipeline de recuperaci√≥n en detalle:
- Indexaci√≥n/documentos: chunking + embeddings (ver `lib/rag/chunking.ts`, `index-document.ts`).
- B√∫squeda: hybrid (vector + lexical) con `useHybrid: true`.
- Re‚Äëranking: `useReranking: true` ajusta la relevancia; ver logs `[RERANK]`.
- Ensamblado de contexto: `buildContextBlock(chunks, maxContextChars)` produce un bloque con citas/sources.

## Prompts de Cleo
- Constructor de sistema: `lib/prompts/index.ts`.
- Variantes: seleccionar con `metadata.systemPromptVariant` (p. ej. "default", "coding", "assistant").
- Overwrite opcional: `metadata.systemPrompt` permite forzar un prompt de sistema espec√≠fico.

## Streaming en la UI
- El endpoint emite eventos SSE: `text-start`, `text-delta` y `finish` con `usage`.
- La UI del chat ya consume estos eventos y renderiza el flujo de texto progresivamente.

Cabeceras usadas: `Content-Type: text/event-stream`, `Cache-Control: no-cache, no-transform`, `Connection: keep-alive`.

## C√≥mo usarlo desde el cliente
- Selecciona el modelo LangChain en el selector de modelos (p. ej., "langchain:multi-model-smart").
- Para activar RAG desde la UI, habilita la opci√≥n de b√∫squeda/knowledge (`enableSearch`).
- Opcional: ajustar temperatura y `systemPromptVariant`.

## Variables de entorno
- GROQ_API_KEY, OPENAI_API_KEY.

## Soluci√≥n de problemas
- No se ve streaming: revisa cabeceras SSE y que el endpoint use `text-delta`.
- 401/keys: comprueba variables GROQ/OPENAI y el mapeo de proveedor‚Äëmodelo.
- RAG no inyecta contexto: confirma `enableSearch=true`, √≠ndices poblados y `maxContextChars` suficiente.
- Prompt no cambia: verifica `systemPromptVariant` o `systemPrompt` en metadata.

### Interpretando los logs
- `[RERANK] ‚úÖ Complete: X ‚Üí Y docs` y `Scores:`: re‚Äëranqueo activo, con tiempos por fase; confirma que RAG est√° encendido.
- `[LangChain] Final system prompt prepared. length: NNNN`: tama√±o del prompt final (sistema+contexto+usuario). Si N es muy alto, reduce `maxContextChars`.
- `‚ö° Processing with LangChain`: inicio de procesamiento del request con resumen de tipo y tama√±o.
- `üß≠/üîç/üéØ ModelRouter`: an√°lisis y decisi√≥n de routing; muestra modelo y fallback.
- `üè≠/‚ôªÔ∏è AgentFactory`: creaci√≥n o reutilizaci√≥n de agentes (reutilizaci√≥n = menor latencia).
- `üöÄ <Agent> processing task`: el agente seleccionado; `useRAG: true/false` indica si inyect√≥ contexto.
- `üì§ Sending request to <modelo>`: llamada al LLM del proveedor.
- `‚úÖ <Agent> processing completed`: incluye tiempos, tokens estimados y costo.
- `‚úÖ LangChain processing complete`: resumen final antes de emitir a la UI.

Si no aparecen logs `[RERANK]`, asegura `enableSearch=true` y que existan documentos indexados para ese `userId`/`documentId`.

## Extensi√≥n
- A√±ade nuevos agentes en `lib/langchain/agents.ts` siguiendo el patr√≥n actual.
- Integra nuevos proveedores LangChain instalando su paquete y extendiendo el router.
- Agrega herramientas/pipelines adicionales sin tocar la UI.

## Par√°metros y opciones soportadas
- `metadata.useRAG`: boolean. Activa recuperaci√≥n de contexto (se mapea desde `options.enableSearch`).
- `metadata.maxContextChars`: n√∫mero. L√≠mite de caracteres del bloque de contexto RAG (default ~6000).
- `metadata.systemPromptVariant`: "default" | "journalism" | "developer" | "reasoning" | "minimal" | "debug".
- `metadata.systemPrompt`: string. Sobrescribe totalmente el prompt de sistema.
- `options.temperature`: n√∫mero. Se pasa al modelo cuando est√° soportado.

## M√©tricas
`MultiModelPipeline.getMetrics()` expone:
- totalRequests, totalCost (estimado), averageResponseTime, modelUsage, costPerRequest y mostUsedModel.
√ötil para dashboards y tuning de routing.
