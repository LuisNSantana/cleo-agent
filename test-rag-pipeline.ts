/**
 * Test script para verificar el pipeline RAG completo:
 * 1. Fireworks + Llama4scout context window
 * 2. Chunking functionality
 * 3. Reranking con OpenAI embeddings
 * 4. Logging mejorado
 */

import { chunkMarkdown, Chunk } from './lib/rag/chunking'
import { defaultReranker, RerankResult } from './lib/rag/reranking'

// Sample document para testing
const SAMPLE_DOCUMENT = `
# Cleo Agent Documentation

## Introduction
Cleo Agent is an advanced AI assistant designed to help developers with coding tasks.
It uses RAG (Retrieval-Augmented Generation) to provide contextual responses.

## RAG Pipeline Components

### Chunking
The chunking system breaks down large documents into manageable pieces.
It preserves markdown structure and maintains context overlap between chunks.
This ensures that important information isn't lost at chunk boundaries.

### Embeddings
We use OpenAI's text-embedding-3-small model for generating vector embeddings.
These embeddings allow semantic similarity search across document chunks.
The embedding dimension is 1536, providing rich semantic representation.

### Vector Storage
Supabase with pgvector extension stores our embeddings.
We use cosine similarity for efficient vector searches.
Hybrid search combines vector similarity with full-text search.

### Reranking
After initial retrieval, we rerank results using embedding similarity.
This improves relevance and ensures the best matches are returned first.
OpenAI embeddings provide consistent quality for reranking.

## Fireworks Integration

### Llama4 Scout Model
Fireworks provides access to Llama4 Scout with extended context windows.
The model supports up to 131K tokens, ideal for processing large documents.
It includes multimodal capabilities (text + vision) for comprehensive analysis.

### Performance Benefits
- High throughput for batch processing
- Competitive pricing for open-source models  
- Multi-cloud deployment options
- Enterprise compliance features

## Best Practices

### Document Processing
1. Clean and preprocess documents before chunking
2. Maintain consistent chunk sizes (600-1000 tokens)
3. Use overlap to preserve context
4. Index metadata for filtering

### Query Optimization
1. Use hybrid search for best recall
2. Apply reranking for precision
3. Filter by user/project for security
4. Cache frequent queries

### Monitoring
1. Track embedding generation latency
2. Monitor vector search performance
3. Log reranking effectiveness
4. Measure end-to-end retrieval time
`

async function testFireworksContextWindow() {
  console.log('\nüî• Testing Fireworks API and Context Window...')
  
  try {
    const response = await fetch('/api/fireworks', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        mode: 'chat',
        model: 'accounts/fireworks/models/llama4-scout-instruct-basic',
        messages: [
          {
            role: 'user',
            content: `Analyze this document and tell me about RAG implementation:\n\n${SAMPLE_DOCUMENT}`
          }
        ]
      })
    })
    
    if (!response.ok) {
      throw new Error(`Fireworks API error: ${response.status}`)
    }
    
    const data = await response.json()
    console.log('‚úÖ Fireworks API Response:', {
      model: data.raw?.model || 'unknown',
      usage: data.raw?.usage || 'no usage info',
      responseLength: data.raw?.choices?.[0]?.message?.content?.length || 0
    })
    
    // Test context window by counting tokens in our sample
    const tokenEstimate = Math.ceil(SAMPLE_DOCUMENT.length / 4)
    console.log(`üìä Context Window Test:`)
    console.log(`   Sample document: ~${tokenEstimate} tokens`)
    console.log(`   Model capacity: 131,072 tokens`)
    console.log(`   Utilization: ${((tokenEstimate / 131072) * 100).toFixed(2)}%`)
    
  } catch (error) {
    console.error('‚ùå Fireworks test failed:', error)
  }
}

async function testChunking(): Promise<Chunk[]> {
  console.log('\nüìÑ Testing Chunking System...')
  
  try {
    const chunks = chunkMarkdown(SAMPLE_DOCUMENT, {
      maxTokens: 400,
      overlapTokens: 50,
      minChunkChars: 200
    })
    
    console.log(`‚úÖ Chunking Results:`)
    console.log(`   Total chunks: ${chunks.length}`)
    console.log(`   Average tokens per chunk: ${Math.round(chunks.reduce((sum: number, c: Chunk) => sum + c.tokenEstimate, 0) / chunks.length)}`)
    
    chunks.forEach((chunk: Chunk, i: number) => {
      console.log(`   Chunk ${i + 1}: ${chunk.tokenEstimate} tokens, ${chunk.text.length} chars`)
      console.log(`   Preview: "${chunk.text.slice(0, 100)}..."`)
    })
    
    return chunks
    
  } catch (error) {
    console.error('‚ùå Chunking test failed:', error)
    return []
  }
}

async function testReranking(chunks: Chunk[]) {
  console.log('\nüîÑ Testing Reranking System...')
  
  if (!chunks.length) {
    console.log('‚ö†Ô∏è  No chunks available for reranking test')
    return
  }
  
  try {
    const query = "How does the RAG pipeline work with embeddings and vector storage?"
    const documents = chunks.map((c: Chunk) => c.text)
    
    console.log(`üîç Query: "${query}"`)
    console.log(`üìö Documents to rerank: ${documents.length}`)
    
    const startTime = Date.now()
    const results = await defaultReranker.rerank(query, documents, {
      topK: 3,
      minScore: 0.1
    })
    const endTime = Date.now()
    
    console.log(`‚úÖ Reranking Results (${endTime - startTime}ms):`)
    results.forEach((result: RerankResult, i: number) => {
      console.log(`   Rank ${i + 1}: Score ${result.score.toFixed(3)}`)
      console.log(`   Text: "${result.text.slice(0, 120)}..."`)
    })
    
    // Test batch reranking
    if (documents.length > 5) {
      console.log('\nüîÑ Testing Batch Reranking...')
      const batchStartTime = Date.now()
      const batchResults = await defaultReranker.rerankBatch(query, documents, 3, { topK: 5 })
      const batchEndTime = Date.now()
      
      console.log(`‚úÖ Batch Results (${batchEndTime - batchStartTime}ms):`)
      console.log(`   Processed ${documents.length} docs in batches of 3`)
      console.log(`   Top ${batchResults.length} results returned`)
    }
    
  } catch (error) {
    console.error('‚ùå Reranking test failed:', error)
  }
}

async function testIntegration() {
  console.log('\nüîó Testing Full Pipeline Integration...')
  
  try {
    // Simulate complete RAG flow
    const chunks = chunkMarkdown(SAMPLE_DOCUMENT, { maxTokens: 300 })
    console.log(`üìÑ Document chunked into ${chunks.length} pieces`)
    
    const query = "What are the best practices for document processing in RAG?"
    const documents = chunks.map((c: Chunk) => c.text)
    
    // Rerank chunks based on query relevance
    const reranked = await defaultReranker.rerank(query, documents, { topK: 2 })
    console.log(`üîÑ Top ${reranked.length} chunks selected after reranking`)
    
    // Combine top chunks for context
    const context = reranked.map((r: RerankResult) => r.text).join('\n\n')
    const contextTokens = Math.ceil(context.length / 4)
    
    console.log(`üìä Context Stats:`)
    console.log(`   Context length: ${context.length} chars`)
    console.log(`   Estimated tokens: ${contextTokens}`)
    console.log(`   Fireworks capacity: ${((contextTokens / 131072) * 100).toFixed(2)}% used`)
    
    // Send to Fireworks with context
    const response = await fetch('/api/fireworks', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        mode: 'chat',
        messages: [
          {
            role: 'system',
            content: `You are an expert on RAG systems. Use this context to answer questions:\n\n${context}`
          },
          {
            role: 'user',
            content: query
          }
        ]
      })
    })
    
    if (response.ok) {
      const data = await response.json()
      const answer = data.raw?.choices?.[0]?.message?.content || 'No response'
      console.log(`‚úÖ Final RAG Response:`)
      console.log(`   "${answer.slice(0, 200)}..."`)
    } else {
      console.error('‚ùå Fireworks API call failed')
    }
    
  } catch (error) {
    console.error('‚ùå Integration test failed:', error)
  }
}

// Main test runner
async function runAllTests() {
  console.log('üöÄ Starting Cleo Agent RAG Pipeline Tests\n')
  console.log('=' .repeat(60))
  
  await testFireworksContextWindow()
  const chunks = await testChunking()
  await testReranking(chunks)
  await testIntegration()
  
  console.log('\n' + '=' .repeat(60))
  console.log('üéâ All tests completed!')
  console.log('\nNext steps:')
  console.log('1. ‚úÖ Fireworks + Llama4scout configured with 131K context')
  console.log('2. ‚úÖ Chunking system working properly')
  console.log('3. ‚úÖ Reranking using OpenAI embeddings')
  console.log('4. üîÑ Ready for production RAG pipeline')
}

// Export for CLI usage
if (require.main === module) {
  runAllTests().catch(console.error)
}

export { runAllTests, testFireworksContextWindow, testChunking, testReranking }
